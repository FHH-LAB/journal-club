# 2019.4.21 周记

## 本周主题 embedding 

### 论文总结



[word2vec Efficient Estimation of Word Representations in Vector Space (Google 2013)](<https://github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BWord2Vec%5D%20Efficient%20Estimation%20of%20Word%20Representations%20in%20Vector%20Space%20(Google%202013).pdf>)

[word2vec Distributed Representations of Words and Phrases and their Compositionality (Google 2013)](https://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%255BWord2Vec%255D%2520Distributed%2520Representations%2520of%2520Words%2520and%2520Phrases%2520and%2520their%2520Compositionality%2520%2528Google%25202013%2529.pdf)

[Word2Vec\] Word2vec Parameter Learning Explained (UMich 2016)](https://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%255BWord2Vec%255D%2520Word2vec%2520Parameter%2520Learning%2520Explained%2520%2528UMich%25202016%2529.pdf)

这三篇分别由浅到深的讲解了word2vce的基本构想到数学模型和实现细节。

了解word2vec的前世今生，simple techniques N-gram 不能处理speech recognition 而用complex models distribute representation with nural network 有较好表现**(outperform)**。distribute representation 有很长历史，早期的popular model architecture for eatimating nueral network language model(NNLM) 学习语言模型的同时学习word vector。另一改进版NNLM先学习word vector在用来训练NNLM。其他model architectures Latent Semantic Analysis(LSA) 和Latent Dirichlet Allocation(LDA)。word2vec focus on distributed representations of words learned by neural networks 因为表现好与其他模型.神经网络系列NNLM->RNNLM->...->CBOW->Skip-gram。**CBOW和Skip-gram的区别如下图：**

![1555858019110](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1555858019110.png)

skip-gram模型目标在于学习quality of vector representation of words and minimizing the computational complexity。

CBOW的每次training复杂度是:
$$
Q = N \times D + D \times log_2 (V)
$$
其中：$N \times D$是projection layer P that dimensionality $N \times D​$ 表示学习词向量的计算量

用binary tree表示词汇表V go down $log_2(V)$  $D \times  log_2(V)$ 表示词向量到输出的计算量

Skip-gram的每次training复杂度是:
$$
Q = C \times (1 \times D + D \times log_2(V))
$$
其中1是每次输入的当前的一个词汇，$C$是maximum distancs of words 用来确定 R in rang <1;C>，R + R words as output。一共训练$C$个words。